{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huang Cong Yao <span style=\"color:red\">103000002</span>\n",
    "\n",
    "# Project 5: Deep Classification\n",
    "\n",
    "## Overview\n",
    "------\n",
    "The project is related to object classification, which we'll implement a classifier on cifar-10 dataset using some deep learning skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "------\n",
    "### 0. Hardware/Software Environment\n",
    "CPU : i7-8700K <br />\n",
    "RAM : 32GB <br />\n",
    "GPU : GTX-1080Ti <br />\n",
    "OS  : Windows / Arch-Linux <br />\n",
    "Building with Tensorflow-GPU version 1.4.0. <br />\n",
    "### 1. Data Loading\n",
    "I implemented a function that auto download and extract the dataset using *urllib.request.urlretrieve()*, before downloading it auto detects whether the dataset is already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(dest_directory, url):\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    file_name = 'cifar-10-binary.tar.gz'\n",
    "    file_path = os.path.join(dest_directory, file_name)\n",
    "    # if have not downloaded yet\n",
    "    if not os.path.exists(file_path):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r%.1f%%' % (float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush() # flush the buffer\n",
    "        print('>> Downloading %s ...' % file_name)\n",
    "        file_path, _ = urllib.request.urlretrieve(url, file_path, _progress)\n",
    "        file_size = os.stat(file_path).st_size\n",
    "        print('\\r>> Total %d bytes' % file_size)\n",
    "    extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        # Open for reading with gzip compression, then extract all\n",
    "        tarfile.open(file_path, 'r:gz').extractall(dest_directory)\n",
    "    print('>> Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can simply put the link into the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Done\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "DEST_DIRECTORY = 'dataset/cifar10'\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "\n",
    "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional Neural Network Model\n",
    "My model is based on the following graph :\n",
    "<center><img style='width: 80%' src='model.png' /></center> \n",
    "And we have some more detail specs :\n",
    "1. All variables are processed in CPU because we want GPU to only focus on calculation. \n",
    "2. Our cost function is *cross entropy* of labels and predictions.\n",
    "3. We use *Weight decay* as our regularization method. The implementation of weight decay is to add a term in the cost function that penalizes the $L^{2}$-norm of the weight matrix at each layer. (see *_variable_with_weight_decay()*)\n",
    "4. The activation function we used in our CNN model is *ReLU*, whose output has no upper bound, which we need a local response normalization to normalize that. The method we use is *Local response normalization*.\n",
    "5. When using gradient descent to update the weights of a neural network, sometimes the weights might move in the wrong direction. Thus, we take a moving average of the weights over a bunch of previous updates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_Model(object):\n",
    "    def __init__(self, batch_size, \n",
    "                 num_classes, \n",
    "                 num_training_example, \n",
    "                 num_epoch_per_decay,\n",
    "                 init_lr,\n",
    "                 moving_average_decay):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_training_example = num_training_example\n",
    "        self.num_epoch_per_decay = num_epoch_per_decay\n",
    "        self.init_lr = init_lr\n",
    "        self.moving_average_decay = moving_average_decay\n",
    "    \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        with tf.device('/cpu:0'):\n",
    "            var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "        return var\n",
    "    \n",
    "    def _variable_with_weight_decay(self, name, shape, stddev, wd=0.0):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "            name: name of the variable\n",
    "            shape: list of ints\n",
    "            stddev: standard deviation of a truncated Gaussian\n",
    "            wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "                decay is not added for this Variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        initializer = tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32)\n",
    "        var = self._variable_on_cpu(name, shape, initializer)\n",
    "        # deal with weight decay\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "        return var\n",
    "    \n",
    "    def inference(self, images):\n",
    "        \"\"\"build the model\n",
    "        Args:\n",
    "            images with shape [batch_size,24,24,3]\n",
    "        Return:\n",
    "            logits with shape [batch_size,10]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('conv_1') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights', [5,5,3,64], 5e-2)\n",
    "            conv = tf.nn.conv2d(images, kernel, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            biases = self._variable_on_cpu('bias', [64], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv_1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        # pool_1\n",
    "        pool_1 = tf.nn.max_pool(conv_1, ksize=[1,3,3,1], strides=[1,2,2,1], \n",
    "                                padding='SAME', name='pool_1') \n",
    "        # norm_1 (local_response_normalization)\n",
    "        norm_1 = tf.nn.lrn(pool_1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm_1')\n",
    "        # conv2\n",
    "        with tf.variable_scope('conv_2') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights', [5, 5, 64, 64], 5e-2)\n",
    "            conv = tf.nn.conv2d(norm_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv_2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        # norm2\n",
    "        norm_2 = tf.nn.lrn(conv_2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm_2')\n",
    "        # pool2\n",
    "        pool_2 = tf.nn.max_pool(norm_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \n",
    "                               padding='SAME', name='pool_2')\n",
    "        # FC_1 (fully-connected layer)\n",
    "        with tf.variable_scope('FC_1') as scope:\n",
    "            flat_features = tf.reshape(pool_2, [self.batch_size, -1])\n",
    "            dim = flat_features.get_shape()[1].value\n",
    "            weights = self._variable_with_weight_decay('weights', [dim, 384], 0.04, 0.004)\n",
    "            biases = self._variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "            FC_1 = tf.nn.relu(tf.matmul(flat_features, weights) + biases, name=scope.name)\n",
    "        # FC_2\n",
    "        with tf.variable_scope('FC_2') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [384, 192], 0.04, 0.004)\n",
    "            biases = self._variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "            FC_2 = tf.nn.relu(tf.matmul(FC_1, weights) + biases, name=scope.name)\n",
    "        with tf.variable_scope('softmax_linear') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [192, self.num_classes],1/192.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_classes], tf.constant_initializer(0.0))\n",
    "            logits = tf.add(tf.matmul(FC_2, weights), biases, name=scope.name)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "        # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "        # decay terms (L2 loss).\n",
    "        return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    \n",
    "    def train(self, total_loss, global_step):\n",
    "        num_batches_per_epoch = self.num_training_example / self.batch_size\n",
    "        decay_steps = int(num_batches_per_epoch * self.num_epoch_per_decay)\n",
    "        # Decay the learning rate exponentially based on the number of steps.\n",
    "        lr = tf.train.exponential_decay(self.init_lr, global_step, decay_steps, \n",
    "                                        decay_rate=0.1, staircase=True)\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        # Track the moving averages of all trainable variables.\n",
    "        # This step just records the moving average weights but not uses them\n",
    "        ema = tf.train.ExponentialMovingAverage(self.moving_average_decay, global_step)\n",
    "        self.ema = ema\n",
    "        variables_averages_op = ema.apply(tf.trainable_variables())\n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parser and iterator\n",
    "We create some parser functions to transform the image into cropped size (and distorted), and an iterator which extract elements from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.data import FixedLengthRecordDataset, Iterator\n",
    "\n",
    "def cifar10_record_distort_parser(record):\n",
    "    ''' Parse the record into label, cropped and distorted image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped and distorted image in the record.\n",
    "  '''\n",
    "    record_bytes = LABEL_BYTES + IMAGE_BYTES\n",
    "    record = tf.decode_raw(record, tf.uint8)\n",
    "    label  = tf.cast(record[0], tf.int32)\n",
    "    \n",
    "    image = tf.reshape(record[1:record_bytes]\n",
    "                       , [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    reshaped_image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "    distorted_image = tf.random_crop(reshaped_image\n",
    "                                     , [IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "    \n",
    "    return label, distorted_image\n",
    "    \n",
    "\n",
    "def cifar10_record_crop_parser(record):\n",
    "    ''' Parse the record into label, cropped image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped image in the record.\n",
    "  '''\n",
    "    record_bytes = LABEL_BYTES + IMAGE_BYTES\n",
    "    record = tf.decode_raw(record, tf.uint8)\n",
    "    label  = tf.cast(record[0], tf.int32)\n",
    "    \n",
    "    image = tf.reshape(record[1:record_bytes]\n",
    "                       , [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    reshaped_image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "    cropped_image = tf.random_crop(reshaped_image\n",
    "                                     , [IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "    cropped_image = tf.image.per_image_standardization(cropped_image)\n",
    "    \n",
    "    return label, cropped_image\n",
    "\n",
    "\n",
    "def cifar10_iterator(filenames, batch_size, cifar10_record_parser):\n",
    "    ''' Create a dataset and return a tf.contrib.data.Iterator \n",
    "    which provides a way to extract elements from this dataset.\n",
    "    -----\n",
    "    Args:\n",
    "        filenames: \n",
    "            a tensor of filenames.\n",
    "        batch_size: \n",
    "            batch size.\n",
    "    Returns:\n",
    "        iterator: \n",
    "            an Iterator providing a way to extract elements from the created dataset.\n",
    "        output_types: \n",
    "            the output types of the created dataset.\n",
    "        output_shapes: \n",
    "            the output shapes of the created dataset.\n",
    "    '''\n",
    "    record_bytes = LABEL_BYTES + IMAGE_BYTES\n",
    "    dataset = FixedLengthRecordDataset(filenames, record_bytes)\n",
    "    dataset = dataset.map(cifar10_record_parser)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(10)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "\n",
    "    return iterator, dataset.output_types, dataset.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameters\n",
    "The following block gathers up all the parameters to be set, we set the batch size to 100, crop image size to 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "BATCH_SIZE = 100\n",
    "NUM_CLASSES = 10 \n",
    "LABEL_BYTES = 1\n",
    "IMAGE_BYTES = 32 * 32 * 3\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pre-processing\n",
    "Using the above functions and models, we setup a training process and its variables, and the training is about to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]\n",
    "\n",
    "filenames_train = tf.constant(training_files)\n",
    "filenames_test = tf.constant(testing_files)\n",
    "\n",
    "iterator_train, types, shapes = cifar10_iterator(filenames_train, BATCH_SIZE, cifar10_record_distort_parser)\n",
    "iterator_test, _, _ = cifar10_iterator(filenames_test, BATCH_SIZE, cifar10_record_crop_parser)\n",
    "\n",
    "next_batch = iterator_train.get_next()\n",
    "\n",
    "# use to handle training and testing\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = Iterator.from_string_handle(handle, types, shapes)\n",
    "labels_images_pairs = iterator.get_next()\n",
    "\n",
    "\n",
    "# CNN model\n",
    "model = CNN_Model(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_training_example=NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
    "    num_epoch_per_decay=350.0,\n",
    "    init_lr=0.1,\n",
    "    moving_average_decay=0.9999)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    labels, images = labels_images_pairs\n",
    "    labels = tf.reshape(labels, [BATCH_SIZE])\n",
    "    images = tf.reshape(images, [BATCH_SIZE, IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, IMAGE_DEPTH])\n",
    "with tf.variable_scope('model'):\n",
    "    logits = model.inference(images)\n",
    "\n",
    "# train\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "total_loss = model.loss(logits, labels)\n",
    "train_op = model.train(total_loss, global_step)\n",
    "# test\n",
    "top_k_op = tf.nn.in_top_k(logits, labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training\n",
    "We train the model for 100 epochs, and print out loss per epoch to see the learning status. When the training process is complete, we store the current model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-5000\n",
      "=== Start Training ===\n",
      "Epoch 11 : Loss = 505.93536, Epoch Time = 8.69s\n",
      "Epoch 12 : Loss = 498.20251, Epoch Time = 8.93s\n",
      "Epoch 13 : Loss = 485.92673, Epoch Time = 8.82s\n",
      "Epoch 14 : Loss = 478.22559, Epoch Time = 8.66s\n",
      "Epoch 15 : Loss = 474.85370, Epoch Time = 8.32s\n",
      "Epoch 16 : Loss = 464.67465, Epoch Time = 8.71s\n",
      "Epoch 17 : Loss = 463.72440, Epoch Time = 8.88s\n",
      "Epoch 18 : Loss = 458.64087, Epoch Time = 8.86s\n",
      "Epoch 19 : Loss = 455.18304, Epoch Time = 9.01s\n",
      "Epoch 20 : Loss = 452.53159, Epoch Time = 8.92s\n",
      "Epoch 21 : Loss = 446.75629, Epoch Time = 8.79s\n",
      "Epoch 22 : Loss = 447.16190, Epoch Time = 8.78s\n",
      "Epoch 23 : Loss = 443.09094, Epoch Time = 8.67s\n",
      "Epoch 24 : Loss = 440.29639, Epoch Time = 8.69s\n",
      "Epoch 25 : Loss = 440.00952, Epoch Time = 8.52s\n",
      "Epoch 26 : Loss = 437.29266, Epoch Time = 8.86s\n",
      "Epoch 27 : Loss = 436.54962, Epoch Time = 8.81s\n",
      "Epoch 28 : Loss = 434.74878, Epoch Time = 9.09s\n",
      "Epoch 29 : Loss = 436.09271, Epoch Time = 8.84s\n",
      "Epoch 30 : Loss = 432.48767, Epoch Time = 8.62s\n",
      "Epoch 31 : Loss = 430.81799, Epoch Time = 8.65s\n",
      "Epoch 32 : Loss = 429.03601, Epoch Time = 8.47s\n",
      "Epoch 33 : Loss = 425.61432, Epoch Time = 9.00s\n",
      "Epoch 34 : Loss = 423.27661, Epoch Time = 8.98s\n",
      "Epoch 35 : Loss = 425.96106, Epoch Time = 8.87s\n",
      "Epoch 36 : Loss = 425.36292, Epoch Time = 8.69s\n",
      "Epoch 37 : Loss = 421.17395, Epoch Time = 8.82s\n",
      "Epoch 38 : Loss = 421.16486, Epoch Time = 8.71s\n",
      "Epoch 39 : Loss = 421.53369, Epoch Time = 8.60s\n",
      "Epoch 40 : Loss = 417.96567, Epoch Time = 8.64s\n",
      "Epoch 41 : Loss = 417.55670, Epoch Time = 8.62s\n",
      "Epoch 42 : Loss = 416.55585, Epoch Time = 8.62s\n",
      "Epoch 43 : Loss = 417.17969, Epoch Time = 8.77s\n",
      "Epoch 44 : Loss = 414.51666, Epoch Time = 8.67s\n",
      "Epoch 45 : Loss = 414.26416, Epoch Time = 8.65s\n",
      "Epoch 46 : Loss = 413.87906, Epoch Time = 8.61s\n",
      "Epoch 47 : Loss = 411.70584, Epoch Time = 8.61s\n",
      "Epoch 48 : Loss = 410.89648, Epoch Time = 8.60s\n",
      "Epoch 49 : Loss = 411.02148, Epoch Time = 8.64s\n",
      "Epoch 50 : Loss = 410.26135, Epoch Time = 8.64s\n",
      "Epoch 51 : Loss = 410.81827, Epoch Time = 8.62s\n",
      "Epoch 52 : Loss = 406.74316, Epoch Time = 8.63s\n",
      "Epoch 53 : Loss = 406.89209, Epoch Time = 8.64s\n",
      "Epoch 54 : Loss = 408.20621, Epoch Time = 8.64s\n",
      "Epoch 55 : Loss = 405.80701, Epoch Time = 8.61s\n",
      "Epoch 56 : Loss = 404.96796, Epoch Time = 8.30s\n",
      "Epoch 57 : Loss = 404.23361, Epoch Time = 8.64s\n",
      "Epoch 58 : Loss = 404.96432, Epoch Time = 8.73s\n",
      "Epoch 59 : Loss = 404.58514, Epoch Time = 8.65s\n",
      "Epoch 60 : Loss = 403.66736, Epoch Time = 8.60s\n",
      "Epoch 61 : Loss = 404.83655, Epoch Time = 8.63s\n",
      "Epoch 62 : Loss = 401.29639, Epoch Time = 8.63s\n",
      "Epoch 63 : Loss = 402.15714, Epoch Time = 8.44s\n",
      "Epoch 64 : Loss = 399.14612, Epoch Time = 8.94s\n",
      "Epoch 65 : Loss = 395.36853, Epoch Time = 9.10s\n",
      "Epoch 66 : Loss = 397.75955, Epoch Time = 8.99s\n",
      "Epoch 67 : Loss = 397.83636, Epoch Time = 8.86s\n",
      "Epoch 68 : Loss = 398.31540, Epoch Time = 8.77s\n",
      "Epoch 69 : Loss = 396.96423, Epoch Time = 8.75s\n",
      "Epoch 70 : Loss = 392.88483, Epoch Time = 8.78s\n",
      "Epoch 71 : Loss = 395.67206, Epoch Time = 8.75s\n",
      "Epoch 72 : Loss = 396.07806, Epoch Time = 8.71s\n",
      "Epoch 73 : Loss = 395.43365, Epoch Time = 8.72s\n",
      "Epoch 74 : Loss = 394.60263, Epoch Time = 8.74s\n",
      "Epoch 75 : Loss = 392.98834, Epoch Time = 8.69s\n",
      "Epoch 76 : Loss = 393.42010, Epoch Time = 8.48s\n",
      "Epoch 77 : Loss = 393.04480, Epoch Time = 8.77s\n",
      "Epoch 78 : Loss = 392.57944, Epoch Time = 8.79s\n",
      "Epoch 79 : Loss = 391.52124, Epoch Time = 8.76s\n",
      "Epoch 80 : Loss = 390.20746, Epoch Time = 8.76s\n",
      "Epoch 81 : Loss = 391.70148, Epoch Time = 8.47s\n",
      "Epoch 82 : Loss = 390.67441, Epoch Time = 8.73s\n",
      "Epoch 83 : Loss = 391.04657, Epoch Time = 8.78s\n",
      "Epoch 84 : Loss = 387.92612, Epoch Time = 8.69s\n",
      "Epoch 85 : Loss = 388.84784, Epoch Time = 8.50s\n",
      "Epoch 86 : Loss = 387.95892, Epoch Time = 8.74s\n",
      "Epoch 87 : Loss = 388.10312, Epoch Time = 8.74s\n",
      "Epoch 88 : Loss = 389.27032, Epoch Time = 8.74s\n",
      "Epoch 89 : Loss = 389.16183, Epoch Time = 8.71s\n",
      "Epoch 90 : Loss = 389.28003, Epoch Time = 8.47s\n",
      "Epoch 91 : Loss = 386.65314, Epoch Time = 8.79s\n",
      "Epoch 92 : Loss = 386.67123, Epoch Time = 8.85s\n",
      "Epoch 93 : Loss = 384.80078, Epoch Time = 8.75s\n",
      "Epoch 94 : Loss = 381.85590, Epoch Time = 8.39s\n",
      "Epoch 95 : Loss = 383.08554, Epoch Time = 8.74s\n",
      "Epoch 96 : Loss = 380.65570, Epoch Time = 8.77s\n",
      "Epoch 97 : Loss = 383.66092, Epoch Time = 8.37s\n",
      "Epoch 98 : Loss = 385.12457, Epoch Time = 8.67s\n",
      "Epoch 99 : Loss = 383.87558, Epoch Time = 8.80s\n",
      "Epoch 100 : Loss = 383.97726, Epoch Time = 8.61s\n",
      "Epoch 101 : Loss = 381.48346, Epoch Time = 8.76s\n",
      "Epoch 102 : Loss = 380.74677, Epoch Time = 8.79s\n",
      "Epoch 103 : Loss = 381.16479, Epoch Time = 8.62s\n",
      "Epoch 104 : Loss = 381.13889, Epoch Time = 8.76s\n",
      "Epoch 105 : Loss = 378.92175, Epoch Time = 8.71s\n",
      "Epoch 106 : Loss = 380.51160, Epoch Time = 8.71s\n",
      "Epoch 107 : Loss = 380.56995, Epoch Time = 8.47s\n",
      "Epoch 108 : Loss = 378.18195, Epoch Time = 8.60s\n",
      "Epoch 109 : Loss = 377.42035, Epoch Time = 8.32s\n",
      "Epoch 110 : Loss = 378.31485, Epoch Time = 8.89s\n",
      "Done Training, Total Epoch Time = 908.03s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 100\n",
    "NUM_BATCH_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN // BATCH_SIZE\n",
    "ckpt_dir = './model/'\n",
    "\n",
    "\n",
    "# train\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    \n",
    "    if (ckpt and ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "        gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "        sess.run(tf.assign(global_step, gs))\n",
    "    else:\n",
    "        # no checkpoint found\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    loss = []\n",
    "    print('======== Start Training ========')\n",
    "    t0 = time.time()\n",
    "    for i in range(NUM_EPOCH):\n",
    "        _loss = [] \n",
    "        sess.run(iterator_train.initializer)\n",
    "        t1 = time.time()\n",
    "        for _ in range(NUM_BATCH_PER_EPOCH):\n",
    "            lbl, img = sess.run(next_batch)\n",
    "            l, _ = sess.run([total_loss, train_op], feed_dict={images: img, labels: lbl})\n",
    "            _loss.append(l)\n",
    "        loss_this_epoch = np.sum(_loss)\n",
    "        gs = global_step.eval()\n",
    "        t2 = time.time()\n",
    "        print(f'Epoch {int(gs/NUM_BATCH_PER_EPOCH)} : Loss = {loss_this_epoch:.5f}, Epoch Time = {t2-t1:.2f}s')\n",
    "        loss.append(loss_this_epoch)\n",
    "        saver.save(sess, ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "  \n",
    "print(f'Done Training, Total Epoch Time = {t2-t0:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Testing\n",
    "For testing, we restore the latest stored model first, and feed the model our test data to calculate its performance using *tf.nn.in_top_k()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-55000\n",
      "Test : Accuracy = 8246/10000 = 0.825, Test Time : 1.06s\n"
     ]
    }
   ],
   "source": [
    "next_test = iterator_test.get_next()\n",
    "variables_to_restore = model.ema.variables_to_restore()\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // BATCH_SIZE\n",
    "        total_sample_count = num_iter * BATCH_SIZE\n",
    "        true_count = 0\n",
    "        sess.run(iterator_test.initializer)\n",
    "        t1 = time.time()\n",
    "        for _ in range(num_iter):\n",
    "            lbl, img = sess.run(next_test)\n",
    "            predictions = sess.run(top_k_op, feed_dict={images: img, labels: lbl})\n",
    "            true_count += np.sum(predictions)\n",
    "        t2 = time.time()\n",
    "        print(f'Test : Accuracy = {true_count}/{total_sample_count} = {true_count / total_sample_count:.3f}, Test Time : {t2-t1:.2f}s')\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    else:\n",
    "        print(\"{}: No model existed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "* Tensorflow-gpu version 1.4\n",
    "* you can run the attached ipynb file \n",
    "* For training, run *train.py*\n",
    "* For testing, run *test.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The accuracy after 110 epochs is about 82.5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
